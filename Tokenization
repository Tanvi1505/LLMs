"Tokenization is a concept of NLP, which is a process of breaking down text into smaller units.
These tokens can be words, subwords, characters, or even sentences, depending on the use case."



import re
import collections

# Step 1: Prepare the initial vocabulary
def get_vocab(text):
    vocab = collections.defaultdict(int)
    for word in text.strip().split():
        word = word.lower()  # Normalize case
        word = re.sub(r'[^\w\s]', '', word)  # Remove punctuation
        vocab[' '.join(list(word)) + ' </w>'] += 1  # Add end-of-word marker
    return vocab

# Step 2: Count pairs of adjacent symbols
def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pairs[(symbols[i], symbols[i+1])] += freq
    return pairs

# Step 3: Merge the most frequent pair
def merge_vocab(pair, vocab):
    pattern = re.escape(' '.join(pair))
    replacement = ''.join(pair)
    new_vocab = {}
    for word in vocab:
        # Replace the most frequent pair in the word
        new_word = re.sub(pattern, replacement, word)
        new_vocab[new_word] = vocab[word]
    return new_vocab

# Step 4: Run BPE
def run_bpe(text, num_merges=10):
    vocab = get_vocab(text)
    print("Initial Vocabulary:\n", vocab, "\n")

    for i in range(num_merges):
        pairs = get_stats(vocab)
        if not pairs:
            break
        best_pair = max(pairs, key=pairs.get)
        print(f"Step {i+1}: Merging {best_pair} (Frequency: {pairs[best_pair]})")
        vocab = merge_vocab(best_pair, vocab)
        print("Updated Vocabulary:\n", vocab, "\n")

    return vocab

# Sample input
text = "low low low low low lower lower newest newest newest newest newest newest widest widest widest"

# Run BPE
final_vocab = run_bpe(text, num_merges=10)
